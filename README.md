# RAGEN: Training Agents by Reinforcing Reasoning


<img src="./public/ragen.png" width="800px" alt="RICO Framework" />
<p align="center" style="font-size: 18px;">
  <strong>RAGEN</strong> leverages reinforcement learning to train <strong>LLM reasoning agents</strong> in interactive, stochastic environments.<br>
  <em>We strongly believe in the future of RL + LLM + Agents. The release is a minimally viable leap forward.</em>
</p>

## Overview

Reinforcement Learning (RL) with rule-based rewards has shown promise in enhancing reasoning capabilities of large language models (LLMs). However, existing approaches have primarily focused on static, single-turn tasks like math reasoning and coding. Extending these methods to agent scenarios introduces two fundamental challenges:

1. **Multi-turn Interactions**: Agents must perform sequential decision-making and react to environment feedback
2. **Stochastic Environments**: Uncertainty where identical actions can lead to different outcomes

RAGEN addresses these challenges through:
- A Markov Decision Process (MDP) formulation for agent tasks
- Reason-Interaction Chain Optimization (RICO) algorithm that optimizes entire trajectory distributions
- Progressive reward normalization strategies to handle diverse, complex environments

## RICO Framework

<img src="./public/rico.png" width="800px" alt="RICO Framework" />

<p align="center" style="font-size: 16px;">
Figure: Reason-Interaction Chain Optimization (RICO) Framework
</p>

RICO consists of two interleaved steps: language model rollout and language model update.

### Language Model Rollout
During rollout, we distinguish between two types of tokens:
* **Environment tokens** (shown in blue): Generated by the simulator/env, including states $s$ and rewards $r$
* **LLM-generated tokens** (shown in red): Including both thinking tokens $t$ and action tokens $a$

The process flow is as follows:
* Given $s_0,A_0,r_0,s_1..s_t$, the LLM generates reasoning-guided responses $A_t$ consisting of a thinking process (`<think>...</think>`) and environment actions (`<ans>...</ans>`)
* The action $a_t$ is extracted from $A_t$ and fed into the simulator to obtain $r_t$ and $s_{t+1}$
* $A_t$, $r_t$ and $s_{t+1}$ are appended to the existing trajectory
* After $k$ rounds of rollout, the model collects the sequence $s_0,A_0,r_0,s_1...s_k$ for optimization

### Language Model Update
During the update phase:
* The model learns to generate complete rollouts with higher rewards given initial states
* Trajectory-level rewards are calculated by parsing $r_0,...r_{k-1}$ from the trajectory tokens
* Final reward computation: $r = {\rm sum}(r_0,...r_{k-1})$ for each rollout
* Policy is updated to jointly optimize reasoning and action strategies

### Reward Normalization Strategies
To address the stochastic, long-horizon nature of agent interactions, we implement three progressive reward normalization strategies:

1. **Absolute Rewards (ARPO)**: Uses raw rewards directly
2. **Batch-Relative Normalization (BRPO)**: Normalizes rewards within each batch
3. **Group-Relative Normalization (GRPO)**: Normalizes rewards within semantically similar groups

### Evaluation Tasks
We evaluate RAGEN on three tasks with increasing complexity:

1. **Sokoban**: Tests multi-turn interaction in a deterministic puzzle environment
2. **Bi-Arm Bandit**: Tests decision-making under stochastic feedback
3. **Frozen Lake**: Combines both multi-turn interaction and stochasticity challenges

## Performance

We evaluate RAGEN across multiple model sizes and configurations. Below are results from our Sokoban experiments using Qwen-2.5-{0.5B, 3B}-{Instruct, None} and DeepSeek-R1-Distill-Qwen-1.5B.

<img src="./public/loss_curve.png" width="800px" alt="Loss curves for different models" />

Key observations:
- Instruct-finetuned models show early advantages but the gap narrows as training progresses
- Larger models (3B) generally outperform smaller models (0.5B), though the advantage is not dramatic
- The R1-distilled 1.5B model initially underperforms compared to 0.5B models
- Training has not yet converged in these experiments

Our analysis reveals two key aspects of LLM agent training with RL:
1. **Prompt diversity**: Balancing observation variety and effective response comparison
2. **Online rollout frequency**: Mediating between training stability and data recency

## Example Trajectories

Visualization of agent reasoning on the Sokoban task:

<p align="center" style="display: flex; justify-content: center; gap: 10px;">
    <img src="./public/step_1.png" width="200px" alt="Step 1" />
    <img src="./public/step_2.png" width="200px" alt="Step 2" />
</p>

The visualizations show how the agent reasons through sequential steps to solve the puzzle.

## Environment Setup
For detailed setup instructions, please check our [documentation](https://ragen-tutorial.readthedocs.io/). Here's a quick start guide:

```bash
# Setup environment and download data (7MB)
bash scripts/setup_ragen.sh
python scripts/download_data.py
```

If this fails, you can follow the manual setup instructions in `scripts/setup_ragen.md`.

## Training Models
Here's how to train models with RAGEN:

### Create data
We provide 10k first-round-observation data for both Sokoban and FrozenLake tasks.

```bash
# Basic data creation
bash scripts/create_data.sh

# Or for research purposes, create more comprehensive data
bash scripts/create_data_full.sh
```

### Export variables and train
We provide default configuration in `verl/trainer/config/ppo_trainer.yaml`. To train:

```bash
bash train.sh sokoban \
    model.experiment_name=new_test

# Override config parameters as needed
bash train.sh sokoban \
    model.experiment_name=new_test_debug \
    training.train_batch_size=128 \
    training.ppo_batch_size=64
```

## Supervised Finetuning (Optional)
For supervised finetuning with LoRA:

1. Create supervised finetuning data:
```bash
bash sft/generate_data.sh <env_type>
```

2. Finetune the model:
```bash
bash sft/finetune_lora.sh <env_type> <num_gpus> <save_path>
```

3. Merge LoRA weights with the base model:
```bash
python sft/utils/merge_lora.py \
    --base_model_name <base_model_name> \
    --lora_model_path <lora_model_path> \
    --output_path <output_path>
```

## Visualization
To visualize agent trajectories:

1. Set visualization parameters in `train.sh`:
```bash
logging.log_images=True
logging.log_image_dir=log/trajectory
logging.log_image_step_size=4
logging.log_n_image_per_batch=32
```

2. View the visualizations:
```bash
cd log/trajectory
python -m http.server 8000
# Access at http://localhost:8000/[EXP_NAME]/step_[STEP_NUM]/trajectory_data_[ID].html
```

3. For proper font rendering:
```bash
sudo apt-get install fonts-noto-cjk
```

4. Download visualization data from wandb:
```python
from ragen.utils.wandb import download_wandb
download_wandb("RUN_ID") # e.g., 9o465jqj
```

## Case Studies
We provide several case studies showing the model's behavior:
- [Reward hacking](https://github.com/ZihanWang314/agent-r1/blob/main/cases/reward_hacking.txt)
- [Challenging moments](https://github.com/ZihanWang314/agent-r1/blob/main/cases/suck_moment.txt)

More case studies will be added to showcase both successful reasoning patterns and failure modes.

## Feedback
We welcome all forms of feedback! Please raise an issue for bugs, questions, or suggestions. This helps our team address common problems efficiently and builds a more productive community.

## Authors
- [Zihan Wang*](https://zihanwang314.github.io/)
- [Kangrui Wang](https://jameskrw.github.io/)
- [Qineng Wang](https://qinengwang-aiden.github.io/)
- [Pingyue Zhang](https://williamzhangsjtu.github.io/)
- [Manling Li†](https://limanling.github.io)

*: Project Lead; †: Advising.
Remaining authors in alphabetical order.

## Acknowledgements
We thank [DeepSeek](https://github.com/deepseek-ai/DeepSeek-R1) for providing the DeepSeek-R1 model and ideas. We thank the [veRL](https://github.com/volcengine/verl) team for their infrastructure. We thank the [TinyZero](https://github.com/Jiayi-Pan/TinyZero) team for their discoveries that inspired our early exploration. We thank Yiping Lu, Runxin Xu, Kyunghyun Cho for insightful discussions.

## Citation
```md
@misc{RAGEN,
  author       = {Zihan Wang and Kangrui Wang and Qineng Wang and Pingyue Zhang and Manling Li},
  title        = {RAGEN: A General-Purpose Reasoning Agent Training Framework},
  year         = {2025},
  organization = {GitHub},
  url          = {https://github.com/ZihanWang314/ragen},
}
```