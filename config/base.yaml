defaults:
  - ppo_trainer # this is a symbolic link to the verl/verl/trainer/config/ppo_trainer.yaml file
  - envs

actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-0.5B-Instruct
  actor:
    ppo_mini_batch_size: 32
    micro_batch_size_per_gpu: 1
    ppo_micro_batch_size_per_gpu: 1
    use_kl_loss: True
    kl_loss_coef: 0.01
    kl_loss_type: mse
    optim:
      betas: [0.9, 0.95]
  ref:
    log_prob_micro_batch_size_per_gpu: 1
  rollout:
    log_prob_micro_batch_size_per_gpu: 1
    tensor_model_parallel_size: 1
    max_model_len: 2048
    prompt_length: 1 # useless. Just put it here
    response_length: 200 # single-turn response length
    enable_kv_cache: True # much faster with this on:) 6.43s vs 10.29s
    enforce_eager: True
    free_cache_engine: ${actor_rollout_ref.rollout.enforce_eager}
    gpu_memory_utilization: 0.5
    enable_chunked_prefill: ${actor_rollout_ref.rollout.enable_kv_cache}
    # max_num_batched_tokens: 8192 # set only when enable_chunked_prefill is true
    val_kwargs:
      do_sample: False
      temperature: 0


critic:
  ppo_micro_batch_size_per_gpu: 1
  model:
    path: ${actor_rollout_ref.model.path}
  optim:
    betas: [0.9, 0.95]

data:
  max_prompt_length: null
  max_response_length: null
  train_batch_size: 128
  val_batch_size: ${data.train_batch_size} # 128
  env_groups: 8
  rollout_n: ${int_div:${data.train_batch_size}, ${data.env_groups}} # NOTE: need to check. equals batch_size / env_groups, different from "n", because this rollout_n determines the number of envs in the same group

trainer:
  project_name: ragen_new
  experiment_name: test
  total_training_steps: 200
  validation_steps: 2 # validation instances = validation_steps * val_batch_size
  val_before_train: True
  n_gpus_per_node: 1
  test_freq: 10
  val_generations_to_log_to_wandb: 20
  logger: [ 'console', 'wandb' ]

agent_proxy:
  max_turn: 5
  rollout_n: ${data.rollout_n}
  action_sep: "||"
  max_actions: 3 # how many actions can be output at most in a single turn
  use_turn_scores: False # important to GAE when applying token-level rewards to token-level advantages. If False, will take the sum of scores as the reward for the last turn.
  score_normalization: "batch_mean" # TODO

es_manager:
  format_penalty: -1.0
  group_size: ${agent_proxy.rollout_n}
  train:
    env_groups: ${data.env_groups} 
    # under the same group, the env config and env seed are ensured to be equal
    group_size: ${es_manager.group_size}
    env_configs:
      tags: ["SimpleSokoban"]
      n_groups: [8] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation
  val:
    env_groups: ${data.val_batch_size}
    group_size: 1 # should be set to 1 because val temperature is set to 0 and same prompt leads to same output
    env_configs:
      tags: ["SimpleSokoban"]
      n_groups: [128] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation

ctx_manager:
  generation: # go to vllm
    gen_config:
      response_length: ${actor_rollout_ref.rollout.response_length}
      temperature: ${actor_rollout_ref.rollout.temperature}
      top_p: ${actor_rollout_ref.rollout.top_p}
      top_k: ${actor_rollout_ref.rollout.top_k}
      kwargs: null