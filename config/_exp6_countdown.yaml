defaults:
  - critic
  - _exp1_bandit

system:
  CUDA_VISIBLE_DEVICES: "0,1,2,3"

ppo_mini_batch_size: 32
micro_batch_size_per_gpu: 8

actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-3B-Instruct

algorithm:
  adv_estimator: gae # it means "no-critic" here. actual reward normalization function is set in agent_proxy.reward_normalization
  # lam: 1
 
trainer:
  project_name: ragen_new
  experiment_name: countdown
  n_gpus_per_node: 4


agent_proxy:
  max_turn: 1
  max_actions_per_turn: 1 # how many actions can be output at most in a single turn
  reward_normalization:
    grouping: "state" 
    method: "identity"

es_manager:
  train:
    env_groups: 8
    # under the same group, the env config and env seed are ensured to be equal
    group_size: 16
    env_configs:
      tags: ["Countdown"] # BanditGeneralizationNoThink
      n_groups: [8] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation
  val:
    env_groups: 128
    group_size: 1 # should be set to 1 because val temperature is set to 0 and same prompt leads to same output
    env_configs:
      tags: ["Countdown"]
      n_groups: [128] # [128] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation