defaults:
  - base

system:
  CUDA_VISIBLE_DEVICES: "2"

ppo_mini_batch_size: 32
micro_batch_size_per_gpu: 32

actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-0.5B-Instruct
  rollout:
    response_length: 200
    val_kwargs:
      do_sample: True
      temperature: 0.5

algorithm:
  adv_estimator: grpo # it means "no-critic" here. actual reward normalization function is set in agent_proxy.reward_normalization
  # lam: 1
 
trainer:
  project_name: ragen_new
  experiment_name: bandit
  n_gpus_per_node: 1


agent_proxy:
  max_turn: 1
  max_actions_per_turn: 1 # how many actions can be output at most in a single turn
  enable_think: True # False -> no think RL
  reward_normalization:
    grouping: "batch" 
    method: "mean"

es_manager:
  format_penalty: -0.1
  train:
    env_groups: 8
    # under the same group, the env config and env seed are ensured to be equal
    group_size: 16
    env_configs:
      tags: ["Bandit"] # BanditGeneralizationNoThink
      n_groups: [8] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation
  val:
    env_groups: 128
    group_size: 1 # should be set to 1 because val temperature is set to 0 and same prompt leads to same output
    env_configs:
      tags: ["Bandit"]
      n_groups: [128] # [128] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation