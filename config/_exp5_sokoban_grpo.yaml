defaults:
  - _exp1_bandit
  # - critic

system:
  CUDA_VISIBLE_DEVICES: "0,1,2,3"

ppo_mini_batch_size: 32
micro_batch_size_per_gpu: 8

actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-3B-Instruct

algorithm:
  adv_estimator: grpo # it means "no-critic" here. actual reward normalization function is set in agent_proxy.reward_normalization
  # lam: 1
 
trainer:
  project_name: ragen_new
  experiment_name: _exp5_sokoban_grpo
  n_gpus_per_node: 4


agent_proxy:
  max_turn: 5
  max_actions_per_turn: 10 # how many actions can be output at most in a single turn
  reward_normalization:
    grouping: "state" 
    method: "identity"

es_manager:
  train:
    env_groups: 8
    group_size: 16
    env_configs:
      tags: ["SimpleSokoban"]
      n_groups: [8] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation
  val:
    env_groups: 128
    group_size: 1 # should be set to 1 because val temperature is set to 0 and same prompt leads to same output
    env_configs:
      tags: ["SimpleSokoban"]
      n_groups: [128] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation
